{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73790fee-6599-4dfa-85da-807153c9fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: GOOGLE_API_KEY not found. Please create a .env file or set the environment variable.\n",
      "--- AI-Driven Data Generator Initialized ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jayavarshini.v\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the .sql or .xlsx schema file:  C:\\\\Users\\\\Jayavarshini.v\\\\Downloads\\\\new_schema.sql\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Reading schema file: 'new_schema.sql'...\n",
      "   ->  Consulting with Gemini to create a smart generation plan...\n",
      "   ->  Gemini has provided a valid generation plan.\n",
      "\n",
      "Please specify the number of rows for each table.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " - Enter the total number of rows for table 'dim_offices':  20\n",
      " - Enter the total number of rows for table 'dim_employees':  30\n",
      " - Enter the total number of rows for table 'dim_products':  40\n",
      " - Enter the total number of rows for table 'dim_customers':  35\n",
      " - Enter the total number of rows for table 'dim_projects':  35\n",
      " - Enter the total number of rows for table 'junc_project_assignments':  50\n",
      " - Enter the total number of rows for table 'fact_sales':  50\n",
      "\n",
      "Enter the batch size for processing (e.g., 10000 for large datasets):  30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Executing generation plan (in batches of 30)...\n",
      "\n",
      "Processing table: 'dim_offices' for 20 rows...\n",
      "   -> Progress: 20 / 20 rows generated.\n",
      "   -> Success. Data for 'dim_offices' saved to 'dim_offices_sample_data.csv'.\n",
      "\n",
      "Processing table: 'dim_employees' for 30 rows...\n",
      "   -> Progress: 30 / 30 rows generated.\n",
      "   -> Success. Data for 'dim_employees' saved to 'dim_employees_sample_data.csv'.\n",
      "\n",
      "Processing table: 'dim_products' for 40 rows...\n",
      "   -> Progress: 30 / 40 rows generated.\n",
      "   -> Progress: 40 / 40 rows generated.\n",
      "   -> Success. Data for 'dim_products' saved to 'dim_products_sample_data.csv'.\n",
      "\n",
      "Processing table: 'dim_customers' for 35 rows...\n",
      "   -> Progress: 30 / 35 rows generated.\n",
      "   -> Progress: 35 / 35 rows generated.\n",
      "   -> Success. Data for 'dim_customers' saved to 'dim_customers_sample_data.csv'.\n",
      "\n",
      "Processing table: 'dim_projects' for 35 rows...\n",
      "   -> Progress: 30 / 35 rows generated.\n",
      "   -> Progress: 35 / 35 rows generated.\n",
      "   -> Success. Data for 'dim_projects' saved to 'dim_projects_sample_data.csv'.\n",
      "\n",
      "Processing table: 'junc_project_assignments' for 50 rows...\n",
      "   -> Progress: 30 / 50 rows generated.\n",
      "   -> Progress: 50 / 50 rows generated.\n",
      "   -> Success. Data for 'junc_project_assignments' saved to 'junc_project_assignments_sample_data.csv'.\n",
      "\n",
      "Processing table: 'fact_sales' for 50 rows...\n",
      "   -> Progress: 30 / 50 rows generated.\n",
      "   -> Progress: 50 / 50 rows generated.\n",
      "   -> Success. Data for 'fact_sales' saved to 'fact_sales_sample_data.csv'.\n",
      "\n",
      "--- Data Generation Process Complete ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script provides an AI-driven solution for generating sample data based on\n",
    "a provided database schema. It accepts either a .sql DDL file or a structured\n",
    ".xlsx file as input. The script utilizes the Google Gemini API to analyze the\n",
    "schema, infer data types and relationships, and create a comprehensive data\n",
    "generation plan. This plan is then executed using the Faker library to produce\n",
    "realistic, referentially-intact sample data in CSV format.\n",
    "\n",
    "The architecture is object-oriented and optimized for scalability by using a\n",
    "batch-processing and streaming approach to handle large datasets without\n",
    "exhausting system memory.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# --- Import required libraries ---\n",
    "import pandas as pd                       \n",
    "from faker import Faker                   \n",
    "import uuid                               \n",
    "import time                              \n",
    "import random                            \n",
    "import datetime                          \n",
    "import json                               \n",
    "import os                                 \n",
    "import google.generativeai as genai       \n",
    "from dotenv import load_dotenv            \n",
    "import inspect                           \n",
    "genai.configure(api_key=\"AIzaSyDQ8kPzvFKt59vF63arz_e2T82Z0E2pIzg\")\n",
    "# --- Configuration and Initialization ---\n",
    "# Load environment variables from a .env file for secure API key management.\n",
    "load_dotenv()\n",
    "try:\n",
    "    # Authenticate with the Google Generative AI service using API key from environment.\n",
    "    GOOGLE_API_KEY = os.environ['GOOGLE_API_KEY']\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "except (KeyError, TypeError):\n",
    "    # Terminate program if the API key is not found or not configured properly.\n",
    "    print(\"ERROR: GOOGLE_API_KEY not found. Please create a .env file or set the environment variable.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "class SchemaReader:\n",
    "    \"\"\"\n",
    "    Handles the reading and initial parsing of schema definition files.\n",
    "    Supports .sql and .xlsx formats.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def from_file(file_path):\n",
    "        \"\"\"\n",
    "        Reads a schema file from the provided path, determines its type,\n",
    "        and returns the raw content for processing by the AIPlanner.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): The local path to the schema file.\n",
    "\n",
    "        Returns:\n",
    "            tuple[str, str]: A tuple containing the schema content as a string\n",
    "                             and the identified file type ('sql' or 'excel_json').\n",
    "        \"\"\"\n",
    "        # Inform the user that the schema file is being read\n",
    "        print(f\"\\nStep 1: Reading schema file: '{os.path.basename(file_path)}'...\")\n",
    "        \n",
    "        # Ensure the provided file exists before proceeding\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found at path: '{file_path}'.\")\n",
    "\n",
    "        # Case 1: If file is a SQL file\n",
    "        if file_path.lower().endswith('.sql'):\n",
    "            file_type = 'sql'\n",
    "            with open(file_path, 'r') as f:              # Open and read entire file\n",
    "                schema_content = f.read()\n",
    "            return schema_content, file_type\n",
    "\n",
    "        # Case 2: If file is an Excel file (.xlsx or .xls)\n",
    "        elif file_path.lower().endswith(('.xlsx', '.xls')):\n",
    "            file_type = 'excel_json'\n",
    "            df = pd.read_excel(file_path).fillna('')     # Read Excel and replace NaN with empty strings\n",
    "            schema_content = df.to_json(orient='records', indent=2)  # Convert to JSON string\n",
    "            return schema_content, file_type\n",
    "\n",
    "        # Case 3: Unsupported file types\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please provide a .sql or .xlsx file.\")\n",
    "\n",
    "\n",
    "class AIPlanner:\n",
    "    \"\"\"\n",
    "    Interfaces with the Google Gemini API to generate a data creation plan\n",
    "    based on a given schema.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"gemini-1.5-flash-latest\"):\n",
    "        \"\"\"Initializes the AI model client with specified model name.\"\"\"\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    def create_generation_plan(self, schema_definition, file_type):\n",
    "        \"\"\"\n",
    "        Constructs a prompt with the schema, sends it to the Gemini API,\n",
    "        and parses the JSON response into a structured generation plan.\n",
    "\n",
    "        Args:\n",
    "            schema_definition (str): The schema content from the SchemaReader.\n",
    "            file_type (str): The type of schema provided ('sql' or 'excel_json').\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: A list of dictionaries, where each dictionary represents\n",
    "                        a table and its complete generation plan.\n",
    "        \"\"\"\n",
    "        print(\"   ->  Consulting with Gemini to create a smart generation plan...\")\n",
    "\n",
    "        # Construct the AI prompt with clear instructions\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert data architect specializing in the Python Faker library. Your task is to create a complete data generation plan from a schema.\n",
    "        Analyze the schema to determine generation order and select the most realistic Faker METHOD for each column.\n",
    "\n",
    "        IMPORTANT: The \"provider\" you choose MUST be a real, callable method on a Faker() instance (e.g., `fake.name()`, `fake.address()`, `fake.email()`).\n",
    "        DO NOT use provider category names like 'internet', 'commerce', or 'finance'.\n",
    "        - For an email column, the correct provider is 'email', NOT 'internet'.\n",
    "        - For a price column, use 'pydecimal', NOT 'finance'.\n",
    "        - For a product name, use 'word' or 'bs', NOT 'commerce'.\n",
    "        - For a date between two points, use 'date_between', NOT 'date_between_dates'.\n",
    "\n",
    "        Your output MUST be a single, valid JSON array. Each element of the array must be an object representing a a table. Each table object must have:\n",
    "        1.  \"table_name\": The name of the table.\n",
    "        2.  \"generation_order\": An integer indicating the order of generation (e.g., 1, 2, 3...).\n",
    "        3.  \"columns\": A list of column objects.\n",
    "\n",
    "        Each column object must have:\n",
    "        1.  \"column_name\": The name of the column.\n",
    "        2.  \"is_primary_key\": A boolean (true/false).\n",
    "        3.  \"foreign_key\": A string referencing another table's primary key (e.g., \"dim_customer.customer_id\") or null.\n",
    "        4.  \"faker\": An object with \"provider\" (a string representing a callable Faker method name) and \"params\" (a JSON object).\n",
    "\n",
    "        Schema Definition ({file_type}):\n",
    "        ---\n",
    "        {schema_definition}\n",
    "        ---\n",
    "\n",
    "        Generate the complete JSON plan now. Do not include any text or formatting outside of the JSON array.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implements retry mechanism (max 3 attempts) in case of API failure\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                # Send prompt to Gemini API\n",
    "                response = self.model.generate_content(prompt)\n",
    "                # Clean up response (remove markdown formatting if present)\n",
    "                json_text = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                \n",
    "                # Parse AI output into JSON\n",
    "                parsed_json = json.loads(json_text)\n",
    "                plan = []\n",
    "                if isinstance(parsed_json, list):\n",
    "                    plan = parsed_json\n",
    "                elif isinstance(parsed_json, dict):\n",
    "                    # Handle if AI wraps list inside a dictionary\n",
    "                    for key, value in parsed_json.items():\n",
    "                        if isinstance(value, list):\n",
    "                            plan = value\n",
    "                            break\n",
    "                \n",
    "                # Validate plan structure\n",
    "                if plan and all('table_name' in t for t in plan):\n",
    "                    print(\"   ->  Gemini has provided a valid generation plan.\")\n",
    "                    return plan\n",
    "                else:\n",
    "                    raise ValueError(\"The AI's plan did not conform to the expected structure.\")\n",
    "            except Exception as e:\n",
    "                # Retry with exponential backoff if error occurs\n",
    "                print(f\"      ->  Gemini API attempt {attempt+1} failed: {e}. Retrying...\")\n",
    "                time.sleep(2**(attempt+1))\n",
    "\n",
    "        # Fail after multiple retries\n",
    "        raise Exception(\"Failed to retrieve a valid generation plan from the AI after multiple attempts.\")\n",
    "\n",
    "\n",
    "class PlanExecutor:\n",
    "    \"\"\"\n",
    "    Executes the AI-generated plan to produce sample data. Manages data\n",
    "    generation, referential integrity, and file output using a scalable\n",
    "    batch-processing approach.\n",
    "    \"\"\"\n",
    "    def __init__(self, plan):\n",
    "        \"\"\"Initializes the executor with the AI-generated plan.\"\"\"\n",
    "        self.plan = sorted(plan, key=lambda x: x['generation_order'])  # Ensure tables are processed in order\n",
    "        self.primary_key_store = {}  # Stores generated PK values for enforcing uniqueness and FK mapping\n",
    "        self.faker_instances = {}    # Cache of Faker instances per locale\n",
    "\n",
    "    def _get_faker_instance(self, locale='en_GB'):\n",
    "        \"\"\"\n",
    "        Retrieves or creates a cached Faker instance for a specific locale\n",
    "        to optimize performance.\n",
    "        \"\"\"\n",
    "        if locale not in self.faker_instances:\n",
    "            self.faker_instances[locale] = Faker(locale)  # Create new instance if not already cached\n",
    "        return self.faker_instances[locale]\n",
    "\n",
    "    def _generate_one_value(self, provider_name, params, fake_instance):\n",
    "        \"\"\"\n",
    "        Generates a single data value using the specified Faker provider and\n",
    "        parameters. Includes pre-processing for special values and parameter validation.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Special case: generate UUID\n",
    "            if provider_name == 'uuid4':\n",
    "                return str(uuid.uuid4())\n",
    "            \n",
    "            # Replace \"today\" string with today's date\n",
    "            processed_params = params.copy()\n",
    "            for key, value in processed_params.items():\n",
    "                if isinstance(value, str) and value.lower() == 'today':\n",
    "                    processed_params[key] = datetime.date.today()\n",
    "            \n",
    "            # Get Faker method dynamically\n",
    "            provider_method = getattr(fake_instance, provider_name)\n",
    "            \n",
    "            # Ensure only valid parameters are passed to Faker method\n",
    "            sig = inspect.signature(provider_method)\n",
    "            valid_params = {k: v for k, v in processed_params.items() if k in sig.parameters}\n",
    "\n",
    "            return provider_method(**valid_params)  # Generate value\n",
    "        except Exception as e:\n",
    "            # Log error and fallback value\n",
    "            print(f\"   -> Error: Data generation failed for provider '{provider_name}'. Details: {e}\")\n",
    "            return \"generation_error\"\n",
    "\n",
    "    def execute(self, row_counts, batch_size=10000):\n",
    "        \"\"\"\n",
    "        Iterates through the generation plan, generating data in batches to\n",
    "        conserve memory. It maintains referential integrity and streams the\n",
    "        output directly to CSV files.\n",
    "        \"\"\"\n",
    "        print(f\"\\nStep 2: Executing generation plan (in batches of {batch_size})...\")\n",
    "        \n",
    "        # Loop over each table rule in the generation plan\n",
    "        for table_rule in self.plan:\n",
    "            table_name = table_rule['table_name']\n",
    "            num_rows = row_counts.get(table_name, 0)\n",
    "\n",
    "            # Skip tables with zero rows requested\n",
    "            if num_rows == 0:\n",
    "                print(f\"\\nSkipping table '{table_name}' as 0 rows were requested.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nProcessing table: '{table_name}' for {num_rows} rows...\")\n",
    "            \n",
    "            output_filename = f\"{table_name}_sample_data.csv\"  # Output CSV file name\n",
    "            is_first_batch = True  # Flag to include header only in first batch\n",
    "            \n",
    "            # Process data in smaller batches to prevent memory exhaustion\n",
    "            for i in range(0, num_rows, batch_size):\n",
    "                batch_data = []  # Temporary storage for current batch\n",
    "                current_batch_size = min(batch_size, num_rows - i)  # Adjust final batch size\n",
    "                \n",
    "                # Generate rows in current batch\n",
    "                for _ in range(current_batch_size):\n",
    "                    row_data = {}\n",
    "                    # Loop over columns in current table\n",
    "                    for col_rule in table_rule['columns']:\n",
    "                        col_name = col_rule['column_name']\n",
    "                        fake = self._get_faker_instance(col_rule.get('faker', {}).get('locale', 'en_GB'))\n",
    "                        \n",
    "                        value = None\n",
    "                        fk_def = col_rule.get('foreign_key')\n",
    "\n",
    "                        # Case 1: Foreign key column - pick value from parent PK store\n",
    "                        if fk_def and isinstance(fk_def, str) and '.' in fk_def:\n",
    "                            ref_key = \".\".join(fk_def.split('.'))\n",
    "                            if ref_key in self.primary_key_store and self.primary_key_store[ref_key]:\n",
    "                                value = random.choice(self.primary_key_store[ref_key])\n",
    "                            else:\n",
    "                                value = \"FK_REFERENCE_NOT_FOUND\"\n",
    "                        else:\n",
    "                            # Case 2: Normal faker column\n",
    "                            provider = col_rule['faker']['provider']\n",
    "                            params = col_rule['faker']['params']\n",
    "\n",
    "                            # Handle invalid faker providers by defaulting to \"word\"\n",
    "                            if not (hasattr(fake, provider) and callable(getattr(fake, provider))):\n",
    "                                print(f\"   ->  AI suggested an invalid provider '{provider}' for '{col_name}'. Using a safe default.\")\n",
    "                                provider = 'word'\n",
    "                                params = {}\n",
    "\n",
    "                            # Generate actual value\n",
    "                            value = self._generate_one_value(provider, params, fake)\n",
    "\n",
    "                        # If column is Primary Key, ensure uniqueness and store for FK references\n",
    "                        if col_rule['is_primary_key']:\n",
    "                            pk_key = f\"{table_name}.{col_name}\"\n",
    "                            if pk_key not in self.primary_key_store:\n",
    "                                self.primary_key_store[pk_key] = []\n",
    "                            \n",
    "                            # Guarantee uniqueness by regenerating if duplicate found\n",
    "                            while value in self.primary_key_store[pk_key]:\n",
    "                                 value = self._generate_one_value(provider, params, fake)\n",
    "                            self.primary_key_store[pk_key].append(value)\n",
    "                        \n",
    "                        # Assign generated value to row\n",
    "                        row_data[col_name] = value\n",
    "                    # Append completed row to batch\n",
    "                    batch_data.append(row_data)\n",
    "\n",
    "                # Convert batch into DataFrame and write/append to CSV file\n",
    "                output_df = pd.DataFrame(batch_data)\n",
    "                if is_first_batch:\n",
    "                    output_df.to_csv(output_filename, index=False, mode='w')  # Write with header\n",
    "                    is_first_batch = False\n",
    "                else:\n",
    "                    output_df.to_csv(output_filename, index=False, mode='a', header=False)  # Append without header\n",
    "\n",
    "                # Show progress update to user\n",
    "                print(f\"   -> Progress: {min(i + batch_size, num_rows)} / {num_rows} rows generated.\")\n",
    "            \n",
    "            print(f\"   -> Success. Data for '{table_name}' saved to '{output_filename}'.\")\n",
    "\n",
    "\n",
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    Main class that orchestrates the data generation workflow by coordinating\n",
    "    the SchemaReader, AIPlanner, and PlanExecutor.\n",
    "    \"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Initializes and runs the end-to-end data generation process based on user input.\n",
    "        \"\"\"\n",
    "        print(\"--- AI-Driven Data Generator Initialized ---\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Ask user for schema file input path\n",
    "            file_input = input(\"Enter the path to the .sql or .xlsx schema file: \").strip()\n",
    "            \n",
    "            # Step 2: Read schema using SchemaReader\n",
    "            reader = SchemaReader()\n",
    "            schema_content, file_type = reader.from_file(file_input)\n",
    "\n",
    "            # Step 3: Generate data creation plan using AIPlanner\n",
    "            planner = AIPlanner()\n",
    "            ai_plan = planner.create_generation_plan(schema_content, file_type)\n",
    "\n",
    "            # Step 4: Ask user for number of rows per table\n",
    "            row_counts = {}\n",
    "            print(\"\\nPlease specify the number of rows for each table.\")\n",
    "            sorted_plan = sorted(ai_plan, key=lambda x: x['generation_order'])\n",
    "            for table_rule in sorted_plan:\n",
    "                table_name = table_rule['table_name']\n",
    "                while True:\n",
    "                    try:\n",
    "                        rows = int(input(f\" - Enter the total number of rows for table '{table_name}': \"))\n",
    "                        row_counts[table_name] = rows\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        print(\"   Please enter a valid integer.\")\n",
    "            \n",
    "            # Step 5: Ask user for batch size\n",
    "            batch_size = int(input(\"\\nEnter the batch size for processing (e.g., 10000 for large datasets): \"))\n",
    "\n",
    "            # Step 6: Execute plan with PlanExecutor\n",
    "            executor = PlanExecutor(ai_plan)\n",
    "            executor.execute(row_counts, batch_size)\n",
    "\n",
    "        except (FileNotFoundError, ValueError, Exception) as e:\n",
    "            # Handle and display errors gracefully\n",
    "            print(f\"\\nAn error occurred during execution: {e}\")\n",
    "        \n",
    "        print(\"\\n--- Data Generation Process Complete ---\")\n",
    "\n",
    "\n",
    "# --- Main Entry Point ---\n",
    "# This block executes when the script is run directly.\n",
    "if __name__ == \"__main__\":\n",
    "    generator = DataGenerator()   # Create an instance of DataGenerator\n",
    "    generator.run()               # Run the main process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67accfab-8df0-4d0b-a552-009e068e1334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
